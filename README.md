# PySpark Day 2 â€” DataFrame Basics (Parquet Pipeline)

This project demonstrates **Day 2 PySpark fundamentals** with a real ETL-style workflow:

* Read Kaggle CSV
* Infer schema
* DataFrame transformations
* Filtering
* Writing output as **Parquet**
* Windows Hadoop (`winutils.exe`) integration

---

## ğŸ“ Project Structure

```text
pyspark-Day2/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ kaggle_dataset.csv
â”‚
â”œâ”€â”€ warehouse/
â”‚   â””â”€â”€ day2_parquet/
â”‚
â”œâ”€â”€ dataframes_basics.py
â”œâ”€â”€ day2_parquet_pipeline.py
â””â”€â”€ spark-env/
```

---

## âš™ï¸ Requirements

* Python 3.9+
* Java 11
* PySpark
* Hadoop binaries (Windows)

---

## ğŸ”§ Environment Setup

### 1. Virtual Environment

```bash
python -m venv spark-env
spark-env\Scripts\activate
pip install pyspark==3.5.1
```

---

## ğŸªŸ Windows Hadoop Setup (Mandatory for Parquet Write)

### Hadoop Path

```text
bin/winutils.exe
bin/hadoop.dll
```

---

## ğŸŒ± Environment Variables

Set **System Variables**:

Add to **PATH**:

Restart system after setting variables.

---

## âœ… Verification

```bash
echo %HADOOP_HOME%
where winutils
winutils
```

Expected output: No errors.

---

## â–¶ï¸ Run Pipeline

```bash
spark-env\Scripts\activate
python day2_parquet_pipeline.py
```

---

## ğŸ“¦ Output

```text
warehouse/day2_parquet/
â”œâ”€â”€ part-00000-xxxx.snappy.parquet
â”œâ”€â”€ part-00001-xxxx.snappy.parquet
â””â”€â”€ _SUCCESS
```

---

## ğŸ” Read Parquet Output

### Using Spark

```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

df = spark.read.parquet("warehouse/day2_parquet")
df.show(20)
```

---

## ğŸ¯ Learning Objectives

* Distributed CSV ingestion
* Schema inference
* DataFrame API usage
* Column transformation
* Filtering
* Parquet storage format
* Windows Spark-Hadoop integration

---

## ğŸ§  Data Engineering Flow

```text
CSV â†’ Spark Read â†’ Schema â†’ Transform â†’ Filter â†’ Parquet Write â†’ Analytics / BI / ML
```

## ğŸš€ Professional Note

Parquet is the **industry-standard analytics format** used by:

* Spark
* Databricks
* Hive
* AWS Athena
* BigQuery
* Snowflake

